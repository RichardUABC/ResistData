# -*- coding: utf-8 -*-
"""Proyecto Final IDD_NN_TB_RIFA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GCm8_nyjyDL0ZtBLtIrze0BNc-goCuki

Proyecto final
"""



import wandb
from wandb.keras import WandbCallback
wandb.login()

#Importar modulos

import os
import keras
import numpy as np  
import pandas as pd 
import matplotlib.pyplot as plt
#%matplotlib inline

from keras.models import Sequential
from keras.layers import Flatten, Dense, Dropout, Embedding, LSTM
from keras import regularizers, layers, preprocessing
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, hamming_loss, matthews_corrcoef, cohen_kappa_score
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import tensorflow as tf

!git clone https://github.com/RichardUABC/ResistData.git

#cargar data set
DataRaw = np.load('/content/ResistData/dataset.npy', allow_pickle=True)
print(type(DataRaw))
print(DataRaw.ndim)
DataRaw

# Tranformar como diccionario
Datadict = DataRaw[()]
print(Datadict)

# Transformar dataframe
DataDf = pd.DataFrame.from_dict(Datadict)
print(DataDf.shape)
DataDf

# promedio  / Max / Min ancho de columna

DataDf.fillna('').astype(str).apply(lambda x:x.str.len()).max()

# observar si los datos estan balanceados

DataDf.groupby('resistant').size().plot.bar()
plt.show()


# Tokenize caracteres a numeros enteros

Datatok = DataDf.copy()
maxlen = 160 # Limita el numero de caracteres 

max_words = 4 # Numero de caracteres del diccionario que considera ATGC
max_features = max_words

tokenizer = Tokenizer(num_words=max_words, char_level=True)
tokenizer.fit_on_texts(list(Datatok['genes']))
sequences = tokenizer.texts_to_sequences(list(Datatok['genes']))
word_index = tokenizer.word_index
Xpad = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post', value=0)

print('Found %s unique tokens.' % len(word_index))
print('word_index', word_index)

# Separa las etiquetas

labels = np.asarray(Datatok['resistant'])
print(Xpad.shape)
print(labels.shape)

# Revisa una muestra

rowNum = 70
print(Datatok['genes'][rowNum])
print(sequences[rowNum])
print(Xpad[rowNum])
print(labels[rowNum])

# Crea datos de entrenamiento y prueba

training_samples = int(Xpad.shape[0] * 0.9)


indices = np.arange(Xpad.shape[0])
np.random.shuffle(indices) 
Xpad = Xpad[indices]
labels = labels[indices]

x_train = Xpad[:training_samples]
y_train = labels[:training_samples]
x_test = Xpad[training_samples: ]
y_test = labels[training_samples: ]

print('x_train', x_train.shape)
print('y_train', y_train.shape)
print('x_test', x_test.shape)
print('y_test', y_test.shape)

# Modelo ... 128 CNN ventana 27 & Bidirectional GRU accuracy = 

model_CNN = Sequential()
model_CNN.add(Embedding(4, 1, input_length=maxlen))
model_CNN.add(layers.Conv1D(128, 27, activation='relu'))
model_CNN.add(layers.MaxPooling1D(9))
model_CNN.add(layers.Dropout(0.5))
model_CNN.add(layers.Conv1D(128, 9, activation='relu'))
model_CNN.add(layers.Dropout(0.5))
model_CNN.add(layers.Bidirectional(layers.GRU(32, dropout=0.2, recurrent_dropout=0.2)))
model_CNN.add(Dense(1, activation='sigmoid'))
model_CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model_CNN.summary()

# Initialize wandb with your project name
run = wandb.init(project='Proyecto_IDD',
                 config={  # and include hyperparameters and metadata
                     "learning_rate": 0.001,
                     "epochs": 10,
                     "batch_size": 32,
                     "loss_function": "binary_crossentropy"                    
                 })
config = wandb.config  # We'll use this to configure our experiment

# Initialize model like you usually do.
tf.keras.backend.clear_session()
#model = model()
model_CNN.summary()

logging_callback = WandbCallback(log_evaluation=True)

# Entrenar el modelo 

history_CNN = model_CNN.fit(x_train, y_train, batch_size=32, epochs = 10,callbacks=[logging_callback])

# Prediccion final 

final_predictions_CNN = model_CNN.predict(x_test)
print(final_predictions_CNN)

# Modifica las prediciones a  0 y 1
# Cutoff point = 0.5

Preds_CNN = final_predictions_CNN.copy()
print(len(Preds_CNN))

Preds_CNN[ np.where( Preds_CNN >= 0.5 ) ] = 1
Preds_CNN[ np.where( Preds_CNN < 0.5 ) ] = 0
print(Preds_CNN)

#MEtricas
print("Accuracy:", accuracy_score(y_test,Preds_CNN))
print("F1 Score:", f1_score(y_test,Preds_CNN))
print("Precicion:", precision_score(y_test,Preds_CNN))
print("Recall:", recall_score(y_test,Preds_CNN))
print("Hamming Loss:", hamming_loss(y_test,Preds_CNN))
print("Cohen Kappa:", cohen_kappa_score(y_test,Preds_CNN))
print("Matthews Coeficient:", matthews_corrcoef(y_test,Preds_CNN))

# Matriz de confusión 

conf_mx = confusion_matrix(y_test, Preds_CNN)

TN = conf_mx[0,0]
FP = conf_mx[0,1]
FN = conf_mx[1,0]
TP = conf_mx[1,1]

print ('TN: ', TN)
print ('FP: ', FP)
print ('FN: ', FN)
print ('TP: ', TP)

recall = TP/(TP+FN)
precision = TP/(TP+FP)

print (recall, precision)

# funcion para graficar 

def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,
                          normalize=False):
    import itertools
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()
    
plot_confusion_matrix(conf_mx, 
                      normalize    = False,
                      target_names = ['resistant', 'sensistive'],
                      title        = "Confusion Matrix ")

pd.DataFrame(history_CNN.history).plot(figsize=(8, 5))

# Entrenar el modelo 

history_val = model_CNN.fit(x_train, y_train, batch_size=32, epochs = 10,callbacks=[logging_callback],validation_split=0.1)

pd.DataFrame(history_val.history).plot(figsize=(8, 5))

# Prediccion final 

final_predictions = model_CNN.predict(x_test)
print(final_predictions)

# Modifica las prediciones a  0 y 1
# Cutoff point = 0.5

Preds_CNN_VAL = final_predictions.copy()
print(len(Preds_CNN_VAL))

Preds_CNN_VAL[ np.where( Preds_CNN_VAL >= 0.5 ) ] = 1
Preds_CNN_VAL[ np.where( Preds_CNN_VAL < 0.5 ) ] = 0
print(Preds_CNN_VAL)

#Metricas
print("Accuracy:", accuracy_score(y_test,Preds_CNN_VAL))
print("F1 Score:", f1_score(y_test,Preds_CNN_VAL))
print("Precicion:", precision_score(y_test,Preds_CNN_VAL))
print("Recall:", recall_score(y_test,Preds_CNN_VAL))
print("Hamming Loss:", hamming_loss(y_test,Preds_CNN_VAL))
print("Cohen Kappa:", cohen_kappa_score(y_test,Preds_CNN_VAL))
print("Matthews Coeficient:", matthews_corrcoef(y_test,Preds_CNN_VAL))

# Matriz de confusión 

conf_mx = confusion_matrix(y_test, Preds_CNN_VAL)

TN = conf_mx[0,0]
FP = conf_mx[0,1]
FN = conf_mx[1,0]
TP = conf_mx[1,1]

print ('TN: ', TN)
print ('FP: ', FP)
print ('FN: ', FN)
print ('TP: ', TP)

recall = TP/(TP+FN)
precision = TP/(TP+FP)

print (recall, precision)

# funcion para graficar 

def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,
                          normalize=False):
    import itertools
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()
    
plot_confusion_matrix(conf_mx, 
                      normalize    = False,
                      target_names = ['resistant', 'sensistive'],
                      title        = "Confusion Matrix ")

model_LSTM = Sequential()
model_LSTM.add(Embedding(4, 1, input_length=maxlen))
model_LSTM.add(layers.Conv1D(128, 27, activation='relu'))
model_LSTM.add(layers.MaxPooling1D(9))
model_LSTM.add(layers.Dropout(0.5))
model_LSTM.add(layers.Conv1D(128, 9, activation='relu'))
model_LSTM.add(layers.Dropout(0.5))
model_LSTM.add(layers.Bidirectional(layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2)))
model_LSTM.add(Dense(1, activation='sigmoid'))
model_LSTM.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model_LSTM.summary()

# Entrenar el modelo 

history_LSTM = model_LSTM.fit(x_train, y_train, batch_size=32, epochs = 10,callbacks=[logging_callback],validation_split=0.1)

pd.DataFrame(history_LSTM.history).plot(figsize=(8, 5))

# Prediccion final 

final_predictions = model_LSTM.predict(x_test)
print(final_predictions)

# Modifica las prediciones a  0 y 1
# Cutoff point = 0.5

Preds_LSTM = final_predictions.copy()
print(len(Preds_LSTM))

Preds_LSTM[ np.where( Preds_LSTM >= 0.5 ) ] = 1
Preds_LSTM[ np.where( Preds_LSTM < 0.5 ) ] = 0
print(Preds_LSTM)

#Metricas
print("Accuracy:", accuracy_score(y_test,Preds_LSTM))
print("F1 Score:", f1_score(y_test,Preds_LSTM))
print("Precicion:", precision_score(y_test,Preds_LSTM))
print("Recall:", recall_score(y_test,Preds_LSTM))
print("Hamming Loss:", hamming_loss(y_test,Preds_LSTM))
print("Cohen Kappa:", cohen_kappa_score(y_test,Preds_LSTM))
print("Matthews Coeficient:", matthews_corrcoef(y_test,Preds_LSTM))

# Matriz de confusión 

conf_mx = confusion_matrix(y_test, Preds_LSTM)

TN = conf_mx[0,0]
FP = conf_mx[0,1]
FN = conf_mx[1,0]
TP = conf_mx[1,1]

print ('TN: ', TN)
print ('FP: ', FP)
print ('FN: ', FN)
print ('TP: ', TP)

recall = TP/(TP+FN)
precision = TP/(TP+FP)
#Sensibilidad = VP / (VP + FN)​
Especificidad = TN / (TN + FP).

print (recall, precision)

# funcion para graficar 

def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,
                          normalize=False):
    import itertools
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()
    
plot_confusion_matrix(conf_mx, 
                      normalize    = False,
                      target_names = ['resistant', 'sensistive'],
                      title        = "Confusion Matrix ")

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix, classification_report,hamming_loss,matthews_corrcoef,roc_curve

RF = RandomForestClassifier(max_depth=2, random_state=0)
history_RF = RF.fit(x_train, y_train)
y_predict = RF.predict(x_test)

print(classification_report(y_test, y_predict))

#Para dibujar todos las variables con su importancia
from matplotlib import pyplot
pyplot.bar(range(len(history_RF.feature_importances_)), history_RF.feature_importances_)
pyplot.show()

# Prediccion final 

final_predictions = RF.predict(x_test)
print(final_predictions)

# Modifica las prediciones a  0 y 1
# Cutoff point = 0.5

Preds_RF = final_predictions.copy()
print(len(Preds_RF))

Preds_RF[ np.where( Preds_RF >= 0.5 ) ] = 1
Preds_RF[ np.where( Preds_RF < 0.5 ) ] = 0
print(Preds_RF)

#Metricas
print("Accuracy:", accuracy_score(y_test,Preds_RF))
print("F1 Score:", f1_score(y_test,Preds_RF))
print("Precicion:", precision_score(y_test,Preds_RF))
print("Recall:", recall_score(y_test,Preds_RF))
print("Hamming Loss:", hamming_loss(y_test,Preds_RF))
print("Cohen Kappa:", cohen_kappa_score(y_test,Preds_RF))
print("Matthews Coeficient:", matthews_corrcoef(y_test,Preds_RF))

# Matriz de confusión 

conf_mx = confusion_matrix(y_test, Preds_RF)

TN = conf_mx[0,0]
FP = conf_mx[0,1]
FN = conf_mx[1,0]
TP = conf_mx[1,1]

print ('TN: ', TN)
print ('FP: ', FP)
print ('FN: ', FN)
print ('TP: ', TP)

recall = TP/(TP+FN)
precision = TP/(TP+FP)

print (recall, precision)

# funcion para graficar 

def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,
                          normalize=False):
    import itertools
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()
    
plot_confusion_matrix(conf_mx, 
                      normalize    = False,
                      target_names = ['resistant', 'sensistive'],
                      title        = "Confusion Matrix ")

from sklearn import svm
from sklearn.svm import SVC

SVM_model =SVC()
SVM_model.fit(x_train,y_train)
pred_SVM_model = SVM_model.predict(x_test)

print(classification_report(y_test,pred_SVM_model))

# Prediccion final 

final_predictions = SVM_model.predict(x_test)
print(final_predictions)

# Modifica las prediciones a  0 y 1
# Cutoff point = 0.5

pred_SVM = final_predictions.copy()
print(len(pred_SVM))

pred_SVM[ np.where( pred_SVM >= 0.5 ) ] = 1
pred_SVM[ np.where( pred_SVM < 0.5 ) ] = 0
print(pred_SVM)

#Metricas
print("Accuracy:", accuracy_score(y_test,pred_SVM))
print("F1 Score:", f1_score(y_test,pred_SVM))
print("Precicion:", precision_score(y_test,pred_SVM))
print("Recall:", recall_score(y_test,pred_SVM))
print("Hamming Loss:", hamming_loss(y_test,pred_SVM))
print("Cohen Kappa:", cohen_kappa_score(y_test,pred_SVM))
print("Matthews Coeficient:", matthews_corrcoef(y_test,pred_SVM))

# Matriz de confusión 

conf_mx = confusion_matrix(y_test, pred_SVM)

TN = conf_mx[0,0]
FP = conf_mx[0,1]
FN = conf_mx[1,0]
TP = conf_mx[1,1]

print ('TN: ', TN)
print ('FP: ', FP)
print ('FN: ', FN)
print ('TP: ', TP)

recall = TP/(TP+FN)
precision = TP/(TP+FP)

print (recall, precision)

# funcion para graficar 

def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,
                          normalize=False):
    import itertools
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()
    
plot_confusion_matrix(conf_mx, 
                      normalize    = False,
                      target_names = ['resistant', 'sensistive'],
                      title        = "Confusion Matrix ")